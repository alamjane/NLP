{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\jane\n",
      "[nltk_data]     alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cricket = \"Cricket is a bat-and-ball game played between two teams of eleven players on a field at the centre of which is a 20-metre (22-yard) pitch with a wicket at each end, each comprising two bails balanced on three stumps. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cricket',\n",
       " 'is',\n",
       " 'a',\n",
       " 'bat-and-ball',\n",
       " 'game',\n",
       " 'played',\n",
       " 'between',\n",
       " 'two',\n",
       " 'teams',\n",
       " 'of',\n",
       " 'eleven',\n",
       " 'players',\n",
       " 'on',\n",
       " 'a',\n",
       " 'field',\n",
       " 'at',\n",
       " 'the',\n",
       " 'centre',\n",
       " 'of',\n",
       " 'which',\n",
       " 'is',\n",
       " 'a',\n",
       " '20-metre',\n",
       " '(',\n",
       " '22-yard',\n",
       " ')',\n",
       " 'pitch',\n",
       " 'with',\n",
       " 'a',\n",
       " 'wicket',\n",
       " 'at',\n",
       " 'each',\n",
       " 'end',\n",
       " ',',\n",
       " 'each',\n",
       " 'comprising',\n",
       " 'two',\n",
       " 'bails',\n",
       " 'balanced',\n",
       " 'on',\n",
       " 'three',\n",
       " 'stumps',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cricket_tokens = word_tokenize(cricket)\n",
    "cricket_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 43)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cricket_tokens), len(cricket_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'a': 4, 'is': 2, 'two': 2, 'of': 2, 'on': 2, 'at': 2, 'each': 2, 'Cricket': 1, 'bat-and-ball': 1, 'game': 1, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in cricket_tokens:\n",
    "    fdist[i] = fdist[i]+1\n",
    "fdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4),\n",
       " ('is', 2),\n",
       " ('two', 2),\n",
       " ('of', 2),\n",
       " ('on', 2),\n",
       " ('at', 2),\n",
       " ('each', 2),\n",
       " ('Cricket', 1),\n",
       " ('bat-and-ball', 1),\n",
       " ('game', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10 = fdist.most_common(10)\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = 'Death is nothing at all,it does not count,I have only slipped away into the next room,nothing has happened'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Death',\n",
       " 'is',\n",
       " 'nothing',\n",
       " 'at',\n",
       " 'all',\n",
       " ',',\n",
       " 'it',\n",
       " 'does',\n",
       " 'not',\n",
       " 'count',\n",
       " ',',\n",
       " 'I',\n",
       " 'have',\n",
       " 'only',\n",
       " 'slipped',\n",
       " 'away',\n",
       " 'into',\n",
       " 'the',\n",
       " 'next',\n",
       " 'room',\n",
       " ',',\n",
       " 'nothing',\n",
       " 'has',\n",
       " 'happened']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_tokens = word_tokenize(poem)\n",
    "poem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Death', 'is'),\n",
       " ('is', 'nothing'),\n",
       " ('nothing', 'at'),\n",
       " ('at', 'all'),\n",
       " ('all', ','),\n",
       " (',', 'it'),\n",
       " ('it', 'does'),\n",
       " ('does', 'not'),\n",
       " ('not', 'count'),\n",
       " ('count', ','),\n",
       " (',', 'I'),\n",
       " ('I', 'have'),\n",
       " ('have', 'only'),\n",
       " ('only', 'slipped'),\n",
       " ('slipped', 'away'),\n",
       " ('away', 'into'),\n",
       " ('into', 'the'),\n",
       " ('the', 'next'),\n",
       " ('next', 'room'),\n",
       " ('room', ','),\n",
       " (',', 'nothing'),\n",
       " ('nothing', 'has'),\n",
       " ('has', 'happened')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.bigrams(poem_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Death', 'is', 'nothing'),\n",
       " ('is', 'nothing', 'at'),\n",
       " ('nothing', 'at', 'all'),\n",
       " ('at', 'all', ','),\n",
       " ('all', ',', 'it'),\n",
       " (',', 'it', 'does'),\n",
       " ('it', 'does', 'not'),\n",
       " ('does', 'not', 'count'),\n",
       " ('not', 'count', ','),\n",
       " ('count', ',', 'I'),\n",
       " (',', 'I', 'have'),\n",
       " ('I', 'have', 'only'),\n",
       " ('have', 'only', 'slipped'),\n",
       " ('only', 'slipped', 'away'),\n",
       " ('slipped', 'away', 'into'),\n",
       " ('away', 'into', 'the'),\n",
       " ('into', 'the', 'next'),\n",
       " ('the', 'next', 'room'),\n",
       " ('next', 'room', ','),\n",
       " ('room', ',', 'nothing'),\n",
       " (',', 'nothing', 'has'),\n",
       " ('nothing', 'has', 'happened')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(poem_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Death', 'is', 'nothing', 'at', 'all'),\n",
       " ('is', 'nothing', 'at', 'all', ','),\n",
       " ('nothing', 'at', 'all', ',', 'it'),\n",
       " ('at', 'all', ',', 'it', 'does'),\n",
       " ('all', ',', 'it', 'does', 'not'),\n",
       " (',', 'it', 'does', 'not', 'count'),\n",
       " ('it', 'does', 'not', 'count', ','),\n",
       " ('does', 'not', 'count', ',', 'I'),\n",
       " ('not', 'count', ',', 'I', 'have'),\n",
       " ('count', ',', 'I', 'have', 'only'),\n",
       " (',', 'I', 'have', 'only', 'slipped'),\n",
       " ('I', 'have', 'only', 'slipped', 'away'),\n",
       " ('have', 'only', 'slipped', 'away', 'into'),\n",
       " ('only', 'slipped', 'away', 'into', 'the'),\n",
       " ('slipped', 'away', 'into', 'the', 'next'),\n",
       " ('away', 'into', 'the', 'next', 'room'),\n",
       " ('into', 'the', 'next', 'room', ','),\n",
       " ('the', 'next', 'room', ',', 'nothing'),\n",
       " ('next', 'room', ',', 'nothing', 'has'),\n",
       " ('room', ',', 'nothing', 'has', 'happened')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(poem_tokens,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('win', 'buy', 'give')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('winning'),pst.stem('buying'),pst.stem('giving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\jane\n",
      "[nltk_data]     alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem = ['cats', 'cacti', 'geese']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats:cat\n",
      "cacti:cactus\n",
      "geese:goose\n"
     ]
    }
   ],
   "source": [
    "for i in words_to_stem:\n",
    "    print(i + ':' + lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos(parts of speech)\n",
    "peace = 'what do yo mean by, I dont believe in God? I talk to him everyday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "peace_tokenize = word_tokenize(peace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jane alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('what', 'WP')]\n",
      "[('do', 'VB')]\n",
      "[('you', 'PRP')]\n",
      "[('mean', 'NN')]\n",
      "[(',', ',')]\n",
      "[('I', 'PRP')]\n",
      "[('dont', 'NN')]\n",
      "[('believe', 'VB')]\n",
      "[('in', 'IN')]\n",
      "[('God', 'NNP')]\n",
      "[('?', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('talk', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('him', 'PRP')]\n",
      "[('everyday', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "for i in peace_tokenize:\n",
    "    print(nltk.pos_tag([i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mary = 'mary had a little lamb, whome she really loved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of speech.\n",
    "# CC coordinating conjunction\n",
    "CD cardinal digit\n",
    "DT determiner\n",
    "EX existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW foreign word\n",
    "IN preposition/subordinating conjunction\n",
    "JJ adjective 'big'\n",
    "JJR adjective, comparative 'bigger'\n",
    "JJS adjective, superlative 'biggest'\n",
    "LS list marker 1)\n",
    "MD modal could, will\n",
    "NN noun, singular 'desk'\n",
    "NNS noun plural 'desks'\n",
    "NNP proper noun, singular 'Harrison'\n",
    "NNPS proper noun, plural 'Americans'\n",
    "PDT predeterminer 'all the kids'\n",
    "POS possessive ending parent\\'s\n",
    "PRP personal pronoun I, he, she\n",
    "PRPdollar possessive pronoun my, his, hers\n",
    "RB adverb very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP particle give up\n",
    "TO to go 'to' the store.\n",
    "UH interjection errrrrrrrm\n",
    "VB verb, base form take\n",
    "VBD verb, past tense took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle taken\n",
    "VBP verb, sing. present, non-3d take\n",
    "VBZ verb, 3rd person sing. present takes\n",
    "WDT wh-determiner which\n",
    "WP wh-pronoun who, what\n",
    "WPdollar possessive wh-pronoun whose\n",
    "WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mary_tokenize = word_tokenize(mary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mary', 'NN')]\n",
      "[('had', 'VBD')]\n",
      "[('a', 'DT')]\n",
      "[('little', 'JJ')]\n",
      "[('lamb', 'NN')]\n",
      "[(',', ',')]\n",
      "[('whome', 'NN')]\n",
      "[('she', 'PRP')]\n",
      "[('really', 'RB')]\n",
      "[('loved', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "for i in mary_tokenize:\n",
    "    print(nltk.pos_tag([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\jane\n",
      "[nltk_data]     alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\jane\n",
      "[nltk_data]     alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# named entity recognition\n",
    "\n",
    "from nltk import ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "john = 'John lives in New York'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_token = word_tokenize(john)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('lives', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "john_tags = nltk.pos_tag(john_token)\n",
    "john_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (PERSON John/NNP) lives/VBZ in/IN (GPE New/NNP York/NNP))\n"
     ]
    }
   ],
   "source": [
    "john_ner = ne_chunk(john_tags)\n",
    "print(john_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\jane\n",
      "[nltk_data]     alam\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'had', 'all', 'with', \"you'll\", 'who', 'd', 'being', 'been', 'theirs', 'which', \"hadn't\", 'again', 'did', 'wouldn', 'just', 'ours', 'didn', 's', 'myself', 'what', \"she's\", 'on', 'his', 'be', 'they', 'll', 'both', 'himself', 'for', 'that', 'too', 'between', \"you'd\", 'so', 'needn', 'of', 'here', 'me', 'or', 'against', 'about', 'should', 'o', 'mightn', \"weren't\", \"that'll\", 'were', 'haven', 'is', 'such', \"mightn't\", 'after', \"haven't\", \"doesn't\", 'once', 'weren', 'few', 'if', \"needn't\", 'are', \"don't\", 'yourself', 'below', 'to', 'other', 'under', 'has', 'into', 'our', \"wouldn't\", 'nor', 'does', 'how', 'you', \"isn't\", 'it', 'out', 'hasn', 'shan', 'over', 'her', 'more', 'when', 'very', 'now', 'its', 'doesn', 'themselves', 'can', \"should've\", 'i', 're', 'and', \"couldn't\", 'why', 'only', 'ain', 'while', \"shouldn't\", \"mustn't\", 'mustn', 'shouldn', 'where', \"didn't\", 'down', 'during', \"you've\", 'those', 'any', 'an', 'not', 'off', 'from', 'until', 'hers', 'further', 'am', 'some', 'then', 'couldn', 'do', \"won't\", 'the', 'isn', 'won', 'ma', 'was', \"shan't\", 't', 'aren', 'each', \"aren't\", 'm', 've', 'having', 'she', 'above', \"hasn't\", 'in', 'through', 'wasn', 'herself', 'ourselves', 'these', 'a', 'my', 'but', 'we', 'your', 'there', 'yourselves', 'y', 'whom', 'him', 'most', 'have', 'this', 'as', 'than', 'yours', 'them', 'doing', 'because', 'at', 'own', \"wasn't\", 'same', \"you're\", 'itself', 'up', \"it's\", 'will', 'by', 'don', 'hadn', 'no', 'he', 'their', 'before'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Mr.Pavan', ',', 'How', 'today', '?', '.', 'Cool', 'got', 'nice', 'job', 'IBM', '.', 'Wow', 'thats', 'awesome', 'car', '.', 'Weather', 'great', '.']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Hi Mr.Pavan , How are you doing today?. Cool you got a nice job at IBM. Wow thats an awesome car. Weather is great.\"\n",
    "words = word_tokenize(example_text)\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\jane\n",
      "[nltk_data]     alam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1:5 And God called the light Day, and the darkness he called Night.', 'And the evening and the morning were the first day.', '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.', '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.', '1:8 And God called the firmament Heaven.', 'And the evening and the\\nmorning were the second day.', '1:9 And God said, Let the waters under the heaven be gathered together\\nunto one place, and let the dry land appear: and it was so.', '1:10 And God called the dry land Earth; and the gathering together of\\nthe waters called he Seas: and God saw that it was good.', '1:11 And God said, Let the earth bring forth grass, the herb yielding\\nseed, and the fruit tree yielding fruit after his kind, whose seed is\\nin itself, upon the earth: and it was so.', '1:12 And the earth brought forth grass, and herb yielding seed after\\nhis kind, and the tree yielding fruit, whose seed was in itself, after\\nhis kind: and God saw that it was good.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "\n",
    "tok = sent_tokenize(sample)\n",
    "\n",
    "print(tok[5:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
